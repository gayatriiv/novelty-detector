# -*- coding: utf-8 -*-
"""nlp project - 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d-m3wwFNYFaqVBi4kDLzsybF-KSyUWYc

Project set-up
"""

!pip install torch transformers sentence-transformers spacy pdfplumber scikit-learn nltk faiss-cpu
!python -m spacy download en_core_web_sm

"""Upload files"""

from google.colab import files
uploaded = files.upload()

"""Pdf extraction"""

import pdfplumber

def extract_text_from_pdf(pdf_path):
    full_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                full_text += text + "\n"
    return full_text

paper_a = extract_text_from_pdf("aerospace-11-00122.pdf")
paper_b = extract_text_from_pdf("aerospace-12-00674-v2.pdf")

print("Paper A length:", len(paper_a))
print("Paper B length:", len(paper_b))

"""Cleaning the files"""

import spacy

nlp = spacy.load("en_core_web_sm")

def split_sentences(text):
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 20]

sentences_a = split_sentences(paper_a)
sentences_b = split_sentences(paper_b)

print("Sentences A:", len(sentences_a))
print("Sentences B:", len(sentences_b))

claim_keywords = [
    "we propose", "we present", "we show", "we demonstrate",
    "our results", "significant", "improves", "outperforms",
    "novel", "first", "contrary", "however", "we conclude"
]

def extract_claims(sentences):
    claims = []
    for s in sentences:
        s_lower = s.lower()
        if any(keyword in s_lower for keyword in claim_keywords):
            claims.append(s)
    return claims

claims_a = extract_claims(sentences_a)
claims_b = extract_claims(sentences_b)

print("Claims A:", len(claims_a))
print("Claims B:", len(claims_b))

"""Embedding model"""

from sentence_transformers import SentenceTransformer

embed_model = SentenceTransformer('all-MiniLM-L6-v2')

emb_a = embed_model.encode(claims_a)
emb_b = embed_model.encode(claims_b)

print("Claims A:", len(claims_a))
print("Claims B:", len(claims_b))

"""Novelty score

"""

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

similarity_matrix = cosine_similarity(emb_a, emb_b)

max_similarities = similarity_matrix.max(axis=1)
novelty_score = np.mean(1 - max_similarities)

print("Novelty Score:", round(float(novelty_score), 3))

"""contradiction detection"""

from transformers import pipeline

nli = pipeline("text-classification", model="facebook/bart-large-mnli", device=0)

pairs = []
for a in claims_a:
    for b in claims_b:
        pairs.append({"text": a, "text_pair": b})

results = nli(pairs, batch_size=16)

contradictions = []

for i, res in enumerate(results):
    if res['label'] == 'CONTRADICTION' and res['score'] > 0.5:

        a = claims_a[i // len(claims_b)]
        b = claims_b[i % len(claims_b)]
        contradictions.append((a, b, res['score']))

print("Number of contradictions:", len(contradictions))

disagreements = []

for idx, res in enumerate(results):
    i, j = index_map[idx]
    similarity = similarity_matrix[i][j]

    if similarity > 0.6 and res['label'] != 'ENTAILMENT':
        disagreements.append((claims_a[i], claims_b[j], res['label'], res['score']))

print("Number of Disagreements:", len(disagreements))

if len(pairs) > 0:
    disagreement_score = len(disagreements) / len(pairs)
else:
    disagreement_score = 0

print("Disagreement Score:", round(disagreement_score, 3))

pairs = []
index_map = []

# only compare semantically similar claims
for i, a in enumerate(claims_a):
    for j, b in enumerate(claims_b):
        if similarity_matrix[i][j] > 0.6:   # similarity filter
            pairs.append({"text": a, "text_pair": b})
            index_map.append((i, j))

# run NLI only on filtered pairs
results = nli(pairs, batch_size=16)

contradictions = []

for idx, res in enumerate(results):
    if res['label'] == 'CONTRADICTION' and res['score'] > 0.5:
        i, j = index_map[idx]
        a = claims_a[i]
        b = claims_b[j]
        contradictions.append((a, b, res['score']))

print("Number of contradictions:", len(contradictions))

"""Contradiction score"""

total_comparisons = len(claims_a) * len(claims_b)
contradiction_score = len(contradictions) / total_comparisons
print("Contradiction Score:", round(contradiction_score, 3))

"""Top contradictions"""

for c in contradictions[:5]:
    print("\n---")
    print("Paper A:", c[0])
    print("Paper B:", c[1])
    print("Score:", round(c[2],3))

from collections import Counter

labels = [r['label'] for r in results]
print(Counter(labels))

if res['label'] == 'CONTRADICTION' and res['score'] > 0.5: